---
title: '<center> <h1>Bidirectional recurrent neural network (LSTM) model in R using TensorFlow</h1></center>'
author: '<center> <h3>Vadim I. Nazarov</h3> </center> <center> <h4><vdm.nazarov@gmail.com></h4> </center>'
output: 
  html_document: 
    theme: spacelab
---

#### Preface
This is an R version of the wonderful TensorFlow example made by [Aymeric Damien](https://github.com/aymericdamien) in [this project](https://github.com/aymericdamien/TensorFlow-Examples/).

Original TensorFlow example notebook: [link](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)

Code for this document: [link](https://github.com/vadimnazarov/tensorflow-r-examples/src/3_NeuralNetworks/bidirectional_rnn.Rmd)

TensorFlow R examples website: [link](https://vadimnazarov.github.io/tensorflow-r-examples/)

TensorFlow R examples GitHub: [link](https://github.com/vadimnazarov/tensorflow-r-examples/)

#### Installation
Installation instructions for TensorFlow R package: [link](https://rstudio.github.io/tensorflow/)

#### Libraries
```{r libs}
library(tensorflow)
```

#### Parameters
```{r params}
learning_rate = .01
n_iters = 100000L
batch_size = 128L
test_size = 1000L
verbose_step = as.integer(n_iters / batch_size / 20)

n_features = 28L
seq_len = 28L
n_hidden = 128L
n_classes = 10L
```

#### Training data
```{r data}
input_data = tf$contrib$learn$datasets$mnist
mnist = input_data$read_data_sets("./tmp/data", one_hot = T)
```

#### TensorFlow graph input
```{r input}
x = tf$placeholder(tf$float32, shape(NULL, n_features, seq_len))
y = tf$placeholder(tf$float32, shape(NULL, n_classes))
```

#### RNN model
```{r model}
birnn_fun <- function (x, weight, bias) {
   # Prepare data shape to match `bidirectional_rnn` function requirements
   # Current data input shape: (batch_size, seq_len, n_features), i.e. 3D cube.
   # Required shape: 'seq_len' tensors list of shape (batch_size, n_features), i.e. sequence of matrices.
  
  # Permuting batch_size and seq_len
  x = tf$transpose(x, c(1L, 0L, 2L))
  # Reshaping to (seq_len*batch_size, n_features)
  x = tf$reshape(x, c(-1L, n_features))
  # Split to get a list of 'seq_len' tensors of shape (batch_size, n_features)
  x = tf$split(0L, seq_len, x)

  # Define forward and backward lstm cells
  lstm_fw_cell = tf$nn$rnn_cell$BasicLSTMCell(n_hidden, forget_bias=1.0)
  lstm_bw_cell = tf$nn$rnn_cell$BasicLSTMCell(n_hidden, forget_bias=1.0)

  # Get lstm cell output - list of output values and states
  output = tf$nn$bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf$float32)
  
  # Linear activation, using rnn inner loop last output
  tf$matmul(output[[1]][[seq_len]], weight) + bias
}

# 2 * n_hidden weights because we have two RNNs here
weight = tf$Variable(tf$random_normal(shape(2 * n_hidden, n_classes)))
bias = tf$Variable(tf$random_normal(shape(n_classes)))

birnn_model = birnn_fun(x, weight, bias)
```

#### Define loss and optimizer
```{r loss}
loss_fun = tf$reduce_mean(tf$nn$softmax_cross_entropy_with_logits(birnn_model, y))
optim_fun = tf$train$AdamOptimizer(learning_rate = learning_rate)$minimize(loss_fun)
```

#### Evaluate the model
```{r eval}
y_true = tf$argmax(birnn_model, 1L) == tf$argmax(y, 1L)
acc_fun = tf$reduce_mean(tf$cast(y_true, tf$float32))
```

#### Initialise all variables
```{r init}
init = tf$initialize_all_variables()
```

#### Launch the graph and print the resulting accuracy
```{r run, fig.align="center", fig.width=16, fig.height=3}
with(tf$Session() %as% sess, {
  sess$run(init)
  
  step = 1
  while (step * batch_size < n_iters) {
    batch_data = mnist$train$next_batch(batch_size)
    batch_data[[1]] = array(batch_data[[1]], c(batch_size, seq_len, n_features))

    sess$run(optim_fun, feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]]))
    if (step %% verbose_step == 0) {
      acc_val = sess$run(acc_fun, feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]]))
      loss_val = sess$run(loss_fun, feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]]))
      cat(paste0("Step: ", format(step, width = 5),
                 " batch.loss=", format(loss_val, width = 10),
                 " accuracy=", format(acc_val, width = 8)), "\n")
    }

    step = step + 1
  }
  
  batch_data = mnist$test$next_batch(test_size)
  test_x = array(batch_data[[1]], c(test_size, seq_len, n_features))
  test_y = batch_data[[2]]
  cat("Testing accuracy on ", test_size, " images=", sess$run(acc_fun, feed_dict = dict(x = test_x, y = test_y)))
})
```