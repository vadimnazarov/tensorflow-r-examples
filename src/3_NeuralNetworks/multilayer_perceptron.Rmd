---
title: '<center> <h1>Multilayer perceptron model in R using TensorFlow</h1></center>'
author: '<center> <h3>Vadim I. Nazarov</h3> </center> <center> <h4><vdm.nazarov@gmail.com></h4> </center>'
output: 
  html_document: 
    theme: spacelab
---

#### Preface
This is an R version of the wonderful TensorFlow example made by [Aymeric Damien](https://github.com/aymericdamien) in [this project](https://github.com/aymericdamien/TensorFlow-Examples/).

Original TensorFlow example notebook: [link](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb)

Code for this document: [link](https://github.com/vadimnazarov/tensorflow-r-examples/3_NeuralNetworks/multilayer_perceptron.Rmd)

TensorFlow R examples website: [link](https://vadimnazarov.github.io/tensorflow-r-examples/)

TensorFlow R examples GitHub: [link](https://github.com/vadimnazarov/tensorflow-r-examples/)

#### Installation
Installation instructions for TensorFlow R package: [link](https://rstudio.github.io/tensorflow/)

#### Libraries
```{r libs}
library(tensorflow)
```

#### Parameters
```{r params}
learning_rate = .001
training_epochs = 15
batch_size = 100L
verbose_step = 1

n_hidden_1 = 256L
n_hidden_2 = 256L
n_features = 784L
n_classes = 10L
```

#### Training data
```{r data}
input_data = tf$contrib$learn$datasets$mnist
mnist = input_data$read_data_sets("./tmp/data", one_hot = T)
```

#### TensorFlow graph input
```{r input}
x = tf$placeholder(tf$float32, shape(NULL, n_features))
y = tf$placeholder(tf$float32, shape(NULL, n_classes))
```

#### Multilayer perceptron model
```{r model}
multilayer_perceptron <- function (x, weights, biases) {
  layer_1 = tf$matmul(x, weights[["h1"]]) + biases[["b1"]]
  layer_1 = tf$nn$relu(layer_1)
  layer_2 = tf$matmul(layer_1, weights[["h2"]]) + biases[["b2"]]
  layer_2 = tf$nn$relu(layer_2)
  tf$matmul(layer_2, weights[["out"]]) + biases[["out"]]
}

weights = list(h1 = tf$Variable(tf$random_normal(shape(n_features, n_hidden_1))), 
               h2 = tf$Variable(tf$random_normal(shape(n_hidden_1, n_hidden_2))), 
               out = tf$Variable(tf$random_normal(shape(n_hidden_2, n_classes))))

biases = list(b1 = tf$Variable(tf$random_normal(shape(n_hidden_1))), 
              b2 = tf$Variable(tf$random_normal(shape(n_hidden_2))), 
              out = tf$Variable(tf$random_normal(shape(n_classes))))

mp_model = multilayer_perceptron(x, weights, biases)
```

#### Define loss and optimizer
```{r loss}
loss_fun = tf$reduce_mean(tf$nn$softmax_cross_entropy_with_logits(mp_model, y))
optim_fun = tf$train$AdamOptimizer(learning_rate = learning_rate)$minimize(loss_fun)
```

#### Initialise all variables
```{r init}
init = tf$initialize_all_variables()
acc = .0
```

#### Launch the L1 distance graph and print the resulting accuracy
```{r run}
with(tf$Session() %as% sess, {
  sess$run(init)
  
  # For every object
  for (epoch in 1:training_epochs) {
    avg_loss = .0
    n_batches = as.integer(mnist$train$num_examples / batch_size)
    for (i_batch in 1:n_batches) {
      batch_data = mnist$train$next_batch(batch_size)
      sess$run(optim_fun, feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]]))
      avg_loss = avg_loss + sess$run(loss_fun, feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]])) / n_batches
    }
    cat(paste0("Epoch: ", format(epoch, width = 2), 
               " avg.loss=", format(avg_loss, width = 10)), "\n")
  }
  
  training_loss = sess$run(loss_fun, feed_dict = dict(x = mnist$test$images, y = mnist$test$labels))
  cat(paste0("Overall loss=", format(training_loss, width = 10)), "\n")

  corr_pred = tf$equal(tf$argmax(mp_model, 1L), tf$argmax(y, 1L))
  acc = tf$reduce_mean(tf$cast(corr_pred, tf$float32))
  cat("Accuracy: ", acc$eval(dict(x = mnist$test$images, y = mnist$test$labels)), "\n")
})
```