---
title: '<center> <h1>Autoencoder model in R using TensorFlow</h1></center>'
author: '<center> <h3>Vadim I. Nazarov</h3> </center> <center> <h4><vdm.nazarov@gmail.com></h4> </center>'
output: 
  html_document: 
    theme: spacelab
---

#### Preface
This is an R version of the wonderful TensorFlow example made by [Aymeric Damien](https://github.com/aymericdamien) in [this project](https://github.com/aymericdamien/TensorFlow-Examples/).

Original TensorFlow example notebook: [link](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb)

Code for this document: [link](https://github.com/vadimnazarov/tensorflow-r-examples/3_NeuralNetworks/autoencoder.Rmd)

TensorFlow R examples website: [link](https://vadimnazarov.github.io/tensorflow-r-examples/)

TensorFlow R examples GitHub: [link](https://github.com/vadimnazarov/tensorflow-r-examples/)

#### Installation
Installation instructions for TensorFlow R package: [link](https://rstudio.github.io/tensorflow/)

#### Libraries
```{r libs}
library(tensorflow)
library(ggplot2)
library(RColorBrewer)
library(gridExtra)
library(reshape2)
```

#### Parameters
```{r params}
learning_rate = .01
n_epochs = 20
batch_size = 256L
verbose_step = 1
to_show = 10

n_features = 784L

n_hidden_1 = 256L
n_hidden_2 = 128L
```

#### Training data
```{r data}
input_data = tf$contrib$learn$datasets$mnist
mnist = input_data$read_data_sets("./tmp/data", one_hot = T)
```

#### TensorFlow graph input
```{r input}
x = tf$placeholder(tf$float32, shape(NULL, n_features))
```

#### Autoencoder model
```{r model}
xxcoder <- function (x, weights, biases) {
  net = tf$nn$sigmoid(tf$matmul(x, weights[["h1"]]) + biases[["b1"]])
  net = tf$nn$sigmoid(tf$matmul(net, weights[["h2"]]) + biases[["b2"]])
  net
}

weights_enc = list(h1 = tf$Variable(tf$random_normal(shape(n_features, n_hidden_1))),
                   h2 = tf$Variable(tf$random_normal(shape(n_hidden_1, n_hidden_2))))
weights_dec = list(h1 = tf$Variable(tf$random_normal(shape(n_hidden_2, n_hidden_1))),
                   h2 = tf$Variable(tf$random_normal(shape(n_hidden_1, n_features))))

biases_enc = list(b1 = tf$Variable(tf$random_normal(shape(n_hidden_1))), 
                  b2 = tf$Variable(tf$random_normal(shape(n_hidden_2))))
biases_dec = list(b1 = tf$Variable(tf$random_normal(shape(n_hidden_1))),
                  b2 = tf$Variable(tf$random_normal(shape(n_features))))

enc_model = xxcoder(x, weights_enc, biases_enc)
dec_model = xxcoder(enc_model, weights_dec, biases_dec)

y_pred = dec_model
y_true = x
```

#### Define loss and optimizer
```{r loss}
loss_fun = tf$reduce_mean((y_true - y_pred) ^ 2)
optim_fun = tf$train$RMSPropOptimizer(learning_rate = learning_rate)$minimize(loss_fun)
```

#### Initialise all variables
```{r init}
init = tf$initialize_all_variables()
```

#### Launch the L1 distance graph and print the resulting accuracy
```{r run, fig.align="center", fig.width=16, fig.height=3}
with(tf$Session() %as% sess, {
  sess$run(init)
  
  n_batches = as.integer(mnist$train$num_examples / batch_size)
  
  for (epoch in 1:n_epochs) {
    for (batch_i in 1:n_batches) {
      batch_data = mnist$train$next_batch(batch_size)
      tmp = sess$run(list(loss_fun, optim_fun), feed_dict = dict(x = batch_data[[1]]))
      loss_val = tmp[1]
    
    }
    
    if (epoch %% verbose_step == 0) {
        cat(paste0("Epoch: ", format(epoch, width = 6), 
                   " loss=", format(loss_val, width = 10)), "\n")
      }
  }
  
  orig_data = mnist$test$images[1:to_show, ]
  enc_dec = sess$run(y_pred, feed_dict = dict(x = orig_data))
  
  do_plot <- function (.data) {
    ggplot() + 
      geom_tile(aes(x = Var1, y = Var2, fill = value), data = .data) + 
      scale_fill_distiller(palette = "YlGnBu") + 
      theme_bw() +
      theme(legend.position = "none",
            axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), 
            axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
      scale_x_discrete(expand = c(0, 0)) +
      scale_y_discrete(expand = c(0, 0)) + 
      coord_fixed()
  }
  
  plots = list()
  for (i in 1:to_show) {
    plots[[i]] = do_plot(melt(matrix(orig_data[i, ], 28, 28)))
    plots[[i+to_show]] = do_plot(melt(matrix(enc_dec[i, ], 28, 28)))
  }
  do.call(grid.arrange, c(plots, list(nrow = 2)))
})
```