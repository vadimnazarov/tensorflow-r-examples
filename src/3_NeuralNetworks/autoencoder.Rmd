---
title: '<center> <h1>Autoencoder model in R using TensorFlow</h1></center>'
author: '<center> <h3>Vadim I. Nazarov</h3> </center> <center> <h4><vdm.nazarov@gmail.com></h4> </center>'
output: 
  html_document: 
    theme: spacelab
---

#### Preface
This is an R version of the wonderful TensorFlow example made by [Aymeric Damien](https://github.com/aymericdamien) in [this project](https://github.com/aymericdamien/TensorFlow-Examples/).

Original TensorFlow example notebook: [link](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb)

Code for this document: [link](https://github.com/vadimnazarov/tensorflow-r-examples/3_NeuralNetworks/autoencoder.Rmd)

TensorFlow R examples website: [link](https://vadimnazarov.github.io/tensorflow-r-examples/)

TensorFlow R examples GitHub: [link](https://github.com/vadimnazarov/tensorflow-r-examples/)

#### Installation
Installation instructions for TensorFlow R package: [link](https://rstudio.github.io/tensorflow/)

#### Libraries
```{r libs}
library(tensorflow)
```

#### Parameters
```{r params}
learning_rate = .01
n_epochs = 20
batch_size = 256L
verbose_step = 1
to_show = 10

n_features = 784L

n_hidden_1 = 256L
n_hidden_2 = 128L
```

#### Training data
```{r data}
input_data = tf$contrib$learn$datasets$mnist
mnist = input_data$read_data_sets("./tmp/data", one_hot = T)
```

#### TensorFlow graph input
```{r input}
x = tf$placeholder(tf$float32, shape(NULL, n_features))
```

#### Autoencoder model
```{r model}
encoder <- function (x, weights, biases) {
  net = tf$nn$sigmoid(tf$matmul(x, weights[["enc_h1"]]) + biases[["enc_b1"]])
  net = tf$nn$sigmoid(tf$matmul(net, weights[["enc_h1"]]) + biases[["enc_b1"]])
  net
}

decoder <- function (x, weights, biases) {
  net = tf$nn$sigmoid(tf$matmul(x, weights[["dec_h1"]]) + biases[["dec_b1"]])
  net = tf$nn$sigmoid(tf$matmul(net, weights[["dec_h1"]]) + biases[["dec_b1"]])
  net
}

weights = list(enc_h1 = tf$Variable(tf$random_normal(shape(n_features, n_hidden_1))),
               enc_h2 = tf$Variable(tf$random_normal(shape(n_hidden_1, n_hidden_2))),
               dec_h1 = tf$Variable(tf$random_normal(shape(n_hidden_2, n_hidden_1))),
               dec_h1 = tf$Variable(tf$random_normal(shape(n_hidden_1, n_features))))

biases = list(enc_h1 = tf$Variable(tf$random_normal(shape(n_hidden_1))), 
              enc_h1 = tf$Variable(tf$random_normal(shape(n_hidden_2))), 
              dec_b1 = tf$Variable(tf$random_normal(shape(n_hidden_1))),
              dec_b1 = tf$Variable(tf$random_normal(shape(n_features))))

enc_model = encoder(x)
dec_model = decoder(enc_model)

y_pred = dec_model
y_true = x
```

#### Define loss and optimizer
```{r loss}
loss_fun = tf$reduce_mean((y_true - y_pred) ^ 2)
optim_fun = tf$train$RMSPropOptimizer(learning_rate = learning_rate)$minimize(loss_fun)
```

#### Initialise all variables
```{r init}
init = tf$initialize_all_variables()
```

#### Launch the L1 distance graph and print the resulting accuracy
```{r run}
with(tf$Session() %as% sess, {
  sess$run(init)
  
  step = 1
  while (step * batch_size < n_iters) {
    batch_data = mnist$train$next_batch(batch_size)
    sess$run(optim_fun, feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]], keep_prob = dropout))
    
    if (step %% verbose_step == 0) {
      tmp = sess$run(list(loss_fun, acc_fun), feed_dict = dict(x = batch_data[[1]], y = batch_data[[2]], keep_prob = 1.))
      loss = tmp[1]
      acc = tmp[2]
      cat(paste0("Iter: ", format(step*batch_size, width = 6), 
               " minibatch loss=", format(loss, width = 10), 
               " train.acc=", format(acc, width = 6)), "\n")
    }
  }

  acc = sess$run(acc_fun, feed_dict = dict(x = mnist$test$images[1:test_size, ], y = mnist$test$labels[1:test_size, ], keep_prob = 1.))
  cat("Accuracy: ", format(acc, width = 6), "\n")
})
```